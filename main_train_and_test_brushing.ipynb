{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['813f', '820c', '8337', '86bd', '891e', '896d', '89f4', '93a2', '94c0', '999e', '9a6b', '9e33', '9eee', 'a153', 'a64e', 'a764', 'aa22', 'aebb', 'b0e8', 'b15a', 'b27d', 'b67b', 'be71']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from annotations.CONSTANTS import *\n",
    "\n",
    "from main_feature_generation import do_generate_feature_vector_and_labels_for_brushing\n",
    "from main_LOSOCV_brushing import evaluate_LOSOCV\n",
    "from main_train_ML_model_and_export import train_and_save_AB_model\n",
    "from main_mORAL_testing import process_mORAL_and_validate, process_mORAL\n",
    "\n",
    "font = {'family': 'calibri',\n",
    "        'weight': 'bold',\n",
    "        'size': 22}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "pids = [d for d in os.listdir(sensor_data_dir) if os.path.isdir(os.path.join(sensor_data_dir, d)) and len(d) == 4]\n",
    "skipped_pids = ['8337', 'a764', 'aebb', 'b0e8']\n",
    "\n",
    "print(pids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory structure for data: \n",
    "\n",
    "## Base directory for sensor data, annotation, and output files\n",
    "CONSTANTS.data_dir = 'your_dir/data/'\n",
    "\n",
    "<br>data_dir</br> contains three folders:\n",
    "<ul>\n",
    "<li>1. annotation</li>\n",
    "<li>2. sensor_data</li>\n",
    "<li>3. features_and_MLresults</li>\n",
    "</ul>\n",
    "\n",
    "### 1. annotation directory\n",
    "This directory contains one annotation file for each video and one metadata file for all the video \n",
    "##### 1a. Configurations.csv \n",
    "contains all the metadata information for each video annotation\n",
    "##### 1b. Annotation files \n",
    "For any video file one annotation file is generated in the form of 'uuuu_YYMMDD_HHmmSS.csv'\n",
    "Here 'uuuu' is the four character user id and YYMMDD_HHmmSS is the video start time.\n",
    "Each CSV file has five columns:\n",
    "<ul>\n",
    "<li> start_offset(sec)  </li>\n",
    "<li>end_offset(sec)\t</li>\n",
    "<li>start_timestamp</li>\n",
    "<li>end_timestamp</li>\n",
    "<li>label</li>\n",
    "</ul>\n",
    "\n",
    "### 2. sensor_data\n",
    "\n",
    "##### 2a. one folder for each participants\n",
    "###### 2a-i. Inside each participant's folder one folder for each day (After daywise splitting)\n",
    "Each of these directory contains 12 files (for each stream of the inertial sensor data):\n",
    "<ul>\n",
    "<li> ax_left_filename = 'left-wrist-accelx.csv' </li>\n",
    "<li> ay_left_filename = 'left-wrist-accely.csv' </li>\n",
    "<li> az_left_filename = 'left-wrist-accelz.csv' </li>\n",
    "<li> gx_left_filename = 'left-wrist-gyrox.csv' </li>\n",
    "<li> gy_left_filename = 'left-wrist-gyroy.csv' </li>\n",
    "<li> gz_left_filename = 'left-wrist-gyroz.csv' </li>\n",
    "<li> ax_right_filename = 'right-wrist-accelx.csv' </li>\n",
    "<li> ay_right_filename = 'right-wrist-accely.csv' </li>\n",
    "<li> az_right_filename = 'right-wrist-accelz.csv' </li>\n",
    "<li> gx_right_filename = 'right-wrist-gyrox.csv' </li>\n",
    "<li> gy_right_filename = 'right-wrist-gyroy.csv' </li>\n",
    "<li> gz_right_filename = 'right-wrist-gyroz.csv' </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "### 3. Output files:\n",
    "CONSTANTS.feature_dir = data_dir + 'features_and_MLresults/'\n",
    "![directory_structure.png](attachment:directory_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Feature Generation\n",
    "\n",
    "#### Generate features and brushing labels for all the participants, i.e., pids\n",
    "#### Export the features and labels as CSV files participantwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['813f', '820c', '8337', '86bd', '891e', '896d', '89f4', '93a2', '94c0', '999e', '9a6b', '9e33', '9eee', 'a153', 'a64e', 'a764', 'aa22', 'aebb', 'b0e8', 'b15a', 'b27d', 'b67b', 'be71']\n",
      "----- Start for  813f\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a96c8c57c36a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdo_generate_feature_vector_and_labels_for_brushing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\projects\\mORAL_signal_processing_and_feature_generation\\main_feature_generation.py\u001b[0m in \u001b[0;36mdo_generate_feature_vector_and_labels_for_brushing\u001b[1;34m(pids)\u001b[0m\n\u001b[0;32m    341\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mday\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbrushed_days_groundtruth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m             \u001b[0mX_brush\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_brush\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_brushing_pid_sid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmbrushing_segmentList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msBrushing_segmentList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_brush\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mY_brush\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 save_as_csv(X_brush, Y_brush, featurenames_brushing, output_brushing_feature_dir,\n",
      "\u001b[1;32m~\\projects\\mORAL_signal_processing_and_feature_generation\\main_feature_generation.py\u001b[0m in \u001b[0;36mprocess_brushing_pid_sid\u001b[1;34m(pid, sid, mbrushing_segmentList, sBrushing_segmentList)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'---------DATA MISSING----'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'#left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAGMO_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     print('Done for', pid, sid, 'Orientations:', left_ori, right_ori, is_new_device, '#pos:',\n\u001b[1;32m--> 313\u001b[1;33m           len([v for v in Y_brush if v[0] == 1]), 'total', len(Y_brush))\n\u001b[0m\u001b[0;32m    314\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_brush\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_brush\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\mORAL_signal_processing_and_feature_generation\\main_feature_generation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'---------DATA MISSING----'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'#left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAGMO_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     print('Done for', pid, sid, 'Orientations:', left_ori, right_ori, is_new_device, '#pos:',\n\u001b[1;32m--> 313\u001b[1;33m           len([v for v in Y_brush if v[0] == 1]), 'total', len(Y_brush))\n\u001b[0m\u001b[0;32m    314\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_brush\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_brush\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "do_generate_feature_vector_and_labels_for_brushing(pids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate different models\n",
    "\n",
    "#### Evaluate different models by Leave-One-Subject_Out_Cross_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate_LOSOCV(pids, skipped_pids, do_feature_selection=True)\n",
    "res_modelwise, AB_res = evaluate_LOSOCV(pids, skipped_pids, do_feature_selection=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LOSOCV Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(15, 10)\n",
    "\n",
    "sns.boxplot(x='Models', y='value', data=res_modelwise, hue='Metrics', width=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Participantwise Ada-Boost model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('participantwise results....')\n",
    "for p, v in AB_res.items():\n",
    "    print(p, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training\n",
    "####     Train the best model (from previous step) and export as pickle file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_filename = 'trained_model_files/brushingAB.model'\n",
    "train_and_save_AB_model(pids, skipped_pids, model_filename=model_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Testing\n",
    "#### Use the trained brushing model and get brushing events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the model and evaluate with Groundtruth\n",
    "process_mORAL_and_validate(pids)\n",
    "\n",
    "# Just testing\n",
    "process_mORAL(pids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
